{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df: pd.DataFrame = pd.read_pickle(\"../data/training_needed_df.pkl\")\n",
    "train_df = train_df.drop(\n",
    "    columns=[\n",
    "        \"MessageID\",\n",
    "        \"ChannelID\",\n",
    "        \"issuerid\",\n",
    "        \"SentimentScore\",\n",
    "        \"MessageText\",\n",
    "        \"llm_issuerid\",\n",
    "        \"llm_SentimentScore\",\n",
    "    ]\n",
    ")\n",
    "train_df = train_df.explode([\"final_ids\", \"final_scores\"])\n",
    "train_df[[f\"embedding_{i}\" for i in range(768)]] = pd.DataFrame(\n",
    "    [x.reshape(-1) for x in train_df[\"embedding\"].tolist()], index=train_df.index\n",
    ")\n",
    "train_df = train_df.drop(columns=[\"embedding\"])\n",
    "train_df.final_ids = train_df.final_ids.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(columns=[\"final_scores\"]).to_numpy()\n",
    "y = (\n",
    "    train_df[\"final_scores\"]\n",
    "    .apply(lambda x: x - 1)\n",
    "    .apply(lambda x: 0 if x < 0 else 4 if x > 4 else x)\n",
    "    .to_numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_name = \"xgboost_study\"\n",
    "\n",
    "\n",
    "class SaveBestModel(xgb.callback.TrainingCallback):\n",
    "    def __init__(self, cvboosters):\n",
    "        self._cvboosters = cvboosters\n",
    "\n",
    "    def after_training(self, model):\n",
    "        self._cvboosters[:] = [cvpack.bst for cvpack in model.cvfolds]\n",
    "        return model\n",
    "\n",
    "\n",
    "def objective(trial: optuna.Trial):\n",
    "    dtrain = xgb.DMatrix(X, label=y)\n",
    "\n",
    "    param = {\n",
    "        \"verbosity\": 0,\n",
    "        \"objective\": \"multi:softmax\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500, log=True),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.0001, 0.1, log=True),\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "    }\n",
    "    param[\"num_class\"] = 5\n",
    "    param[\"device\"] = \"cuda:1\"\n",
    "\n",
    "    if param[\"booster\"] == \"gbtree\" or param[\"booster\"] == \"dart\":\n",
    "        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 9)\n",
    "        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "    if param[\"booster\"] == \"dart\":\n",
    "        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "\n",
    "    cvboosters = []\n",
    "    savemodel_callback = SaveBestModel(cvboosters)\n",
    "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"test-auc\")\n",
    "    history = xgb.cv(\n",
    "        param, dtrain, num_boost_round=100, callbacks=[pruning_callback, savemodel_callback]\n",
    "    )\n",
    "\n",
    "    with open(\"../models/{}_{}.pickle\".format(study_name, trial.number), \"wb\") as fout:\n",
    "        pickle.dump(cvboosters[-1], fout)\n",
    "\n",
    "    mean_auc = history[\"test-auc-mean\"].values[-1]\n",
    "    return mean_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruner = optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    "study = optuna.create_study(\n",
    "    pruner=pruner,\n",
    "    direction=\"maximize\",\n",
    "    study_name=study_name,\n",
    "    storage=\"sqlite:///optuna_study.db\",\n",
    "    load_if_exists=True,\n",
    ")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
