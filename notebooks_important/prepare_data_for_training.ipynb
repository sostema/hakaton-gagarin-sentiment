{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from natasha import (\n",
    "    Doc,\n",
    "    MorphVocab,\n",
    "    NamesExtractor,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsNERTagger,\n",
    "    NewsSyntaxParser,\n",
    "    Segmenter,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "\n",
    "names_extractor = NamesExtractor(morph_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "# Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output: torch.Tensor, attention_mask: torch.Tensor):\n",
    "    token_embeddings = model_output[\n",
    "        0\n",
    "    ]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n",
    "        input_mask_expanded.sum(1), min=1e-9\n",
    "    )\n",
    "\n",
    "\n",
    "def get_embs(text: str, model: AutoModel, tokenizer: AutoTokenizer):\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    encoded_input = encoded_input.to(model.device)\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Perform pooling. In this case, average pooling\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input[\"attention_mask\"])\n",
    "    return sentence_embeddings\n",
    "\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    ")\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    ").cuda(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"labeled_data_v0.1.pickle\", \"rb\") as handle:\n",
    "    b = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_texts = pd.read_pickle(\"../data/sentiment_texts.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_texts[\"MessageID_copy\"] = sentiment_texts[\"MessageID\"].copy()\n",
    "sentiment_texts[\"ChannelID_copy\"] = sentiment_texts[\"ChannelID\"].copy()\n",
    "aggregated_df = sentiment_texts.groupby([\"MessageID_copy\", \"ChannelID_copy\"]).agg(list)\n",
    "only_needed_df = aggregated_df[\n",
    "    [\"MessageID\", \"ChannelID\", \"issuerid\", \"SentimentScore\", \"MessageText\"]\n",
    "]\n",
    "only_needed_df[[\"MessageID\", \"ChannelID\", \"MessageText\"]] = only_needed_df[\n",
    "    [\"MessageID\", \"ChannelID\", \"MessageText\"]\n",
    "].applymap(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_issuer_map():\n",
    "    names_n_synonyms_df = pd.read_excel(\"../data/names and synonyms.xlsx\")\n",
    "    names_n_synonyms_df[\"one_string\"] = names_n_synonyms_df.iloc[:, 2:].apply(\n",
    "        lambda row: \" \".join(row.dropna().tolist()).lower(), axis=1\n",
    "    )\n",
    "    mapper = names_n_synonyms_df.set_index(\"issuerid\")[\"one_string\"]\n",
    "    return mapper\n",
    "\n",
    "\n",
    "def get_company_ids(text, mapper):\n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "    for token in doc.tokens:\n",
    "        token.lemmatize(morph_vocab)\n",
    "    doc.parse_syntax(syntax_parser)\n",
    "    doc.tag_ner(ner_tagger)\n",
    "\n",
    "    for span in doc.spans:\n",
    "        span.normalize(morph_vocab)\n",
    "\n",
    "    companies = []\n",
    "    for x in {_.normal for _ in doc.spans}:\n",
    "        found_ones = mapper[mapper.str.contains(x.lower())].index.tolist()\n",
    "        companies.extend(found_ones)\n",
    "    return companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = get_issuer_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errs = 0\n",
    "\n",
    "embeddings = []\n",
    "issuer_ids = []\n",
    "scores = []\n",
    "\n",
    "for ind, row in tqdm(only_needed_df.iterrows()):\n",
    "    try:\n",
    "        text_emb = get_embs(row[\"MessageText\"], model, tokenizer)\n",
    "        embeddings.append(text_emb.cpu().detach().numpy())\n",
    "        iids = []\n",
    "        sc = []\n",
    "        for r in b:\n",
    "            if row.MessageText == r[\"corpus\"]:\n",
    "                for entity in r[\"entities\"]:\n",
    "                    try:\n",
    "                        found_issuer_ids = mapper[\n",
    "                            mapper.str.contains(entity[\"company\"].lower())\n",
    "                        ].index.tolist()\n",
    "                        if found_issuer_ids:\n",
    "                            target = int(entity[\"score\"])\n",
    "                            iids.extend(found_issuer_ids)\n",
    "                            sc.extend([target] * len(found_issuer_ids))\n",
    "                    except Exception as e:\n",
    "                        errs += 1\n",
    "                        print(e)\n",
    "                break\n",
    "        issuer_ids.append(iids)\n",
    "        scores.append(sc)\n",
    "    except Exception as e:\n",
    "        errs += 1\n",
    "        print(e)\n",
    "print(errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_needed_df[\"embedding\"] = embeddings\n",
    "only_needed_df[\"llm_issuerid\"] = issuer_ids\n",
    "only_needed_df[\"llm_SentimentScore\"] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x = []\n",
    "new_xs = []\n",
    "for x, xs, y, ys in zip(\n",
    "    only_needed_df[\"issuerid\"],\n",
    "    only_needed_df[\"SentimentScore\"],\n",
    "    only_needed_df[\"llm_issuerid\"],\n",
    "    only_needed_df[\"llm_SentimentScore\"],\n",
    "    strict=True,\n",
    "):\n",
    "    new_x.append(x)\n",
    "    new_xs.append(xs)\n",
    "    for _y, _ys in zip(y, ys, strict=True):\n",
    "        if _y not in x:\n",
    "            new_x[-1].append(_y)\n",
    "            new_xs[-1].append(_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_needed_df[\"final_ids\"] = new_x\n",
    "only_needed_df[\"final_scores\"] = new_xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_needed_df.to_pickle(\"../data/training_needed_df.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
