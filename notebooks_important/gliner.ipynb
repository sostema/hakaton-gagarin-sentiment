{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_texts_df: pd.DataFrame = pd.read_pickle(\"../data/sentiment_texts.pkl\")\n",
    "names_and_synonyms_df = pd.read_excel(\"../data/names_n_synonyms.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_and_synonyms_df[\"NAMINGS\"] = names_and_synonyms_df.iloc[:, 3:].apply(\n",
    "    lambda row: row.dropna().tolist(), axis=1\n",
    ")\n",
    "names_and_synonyms_df = names_and_synonyms_df.drop(columns=names_and_synonyms_df.columns[2:-1])\n",
    "names_and_synonyms_df = names_and_synonyms_df.set_index(\"issuerid\")\n",
    "namings = names_and_synonyms_df[\"NAMINGS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_texts_df[\"MessageID_copy\"] = sentiment_texts_df[\"MessageID\"]\n",
    "sentiment_texts_df[\"MessageID_copy\"] = sentiment_texts_df[\"MessageID\"]\n",
    "sentiment_texts_df = (\n",
    "    sentiment_texts_df.groupby(\"MessageID_copy\")\n",
    "    .agg(list)\n",
    "    .map(lambda x: x[0] if (len(x) == 1) else x)\n",
    "    .drop_duplicates(subset=[\"MessageID\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "sentiment_texts_df[[\"issuerid\", \"SentimentScore\"]] = sentiment_texts_df[\n",
    "    [\"issuerid\", \"SentimentScore\"]\n",
    "].map(lambda x: x if isinstance(x, list) else [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gliner import GLiNER\n",
    "\n",
    "model = GLiNER.from_pretrained(\"urchade/gliner_multi-v2.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 429/6889 [03:29<52:34,  2.05it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m     message_id \u001b[38;5;241m=\u001b[39m message_id[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     11\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompany\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 12\u001b[0m entities \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_entities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entity \u001b[38;5;129;01min\u001b[39;00m entities:\n\u001b[1;32m     14\u001b[0m     entity_text \u001b[38;5;241m=\u001b[39m entity[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/dev/hakaton-gagarin-sentiment_interface/.venv/lib/python3.11/site-packages/gliner/model.py:277\u001b[0m, in \u001b[0;36mGLiNER.predict_entities\u001b[0;34m(self, text, labels, flat_ner, threshold)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_entities\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, labels, flat_ner\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m):\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_predict_entities\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_ner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_ner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/dev/hakaton-gagarin-sentiment_interface/.venv/lib/python3.11/site-packages/gliner/model.py:305\u001b[0m, in \u001b[0;36mGLiNER.batch_predict_entities\u001b[0;34m(self, texts, labels, flat_ner, threshold)\u001b[0m\n\u001b[1;32m    303\u001b[0m input_x \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenized_text\u001b[39m\u001b[38;5;124m\"\u001b[39m: tk, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m} \u001b[38;5;28;01mfor\u001b[39;00m tk \u001b[38;5;129;01min\u001b[39;00m all_tokens]\n\u001b[1;32m    304\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(input_x, labels)\n\u001b[0;32m--> 305\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_ner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_ner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m all_entities \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(outputs):\n",
      "File \u001b[0;32m~/dev/hakaton-gagarin-sentiment_interface/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/hakaton-gagarin-sentiment_interface/.venv/lib/python3.11/site-packages/gliner/model.py:258\u001b[0m, in \u001b[0;36mGLiNER.predict\u001b[0;34m(self, x, flat_ner, threshold)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, flat_ner\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m):\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m--> 258\u001b[0m     local_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_score_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m     probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(local_scores)\n\u001b[1;32m    261\u001b[0m     spans \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/dev/hakaton-gagarin-sentiment_interface/.venv/lib/python3.11/site-packages/gliner/model.py:231\u001b[0m, in \u001b[0;36mGLiNER.compute_score_eval\u001b[0;34m(self, x, device)\u001b[0m\n\u001b[1;32m    228\u001b[0m tokens_p \u001b[38;5;241m=\u001b[39m [entity_prompt \u001b[38;5;241m+\u001b[39m tokens \u001b[38;5;28;01mfor\u001b[39;00m tokens \u001b[38;5;129;01min\u001b[39;00m x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m    229\u001b[0m seq_length_p \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m prompt_entity_length\n\u001b[0;32m--> 231\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_rep_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_length_p\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m word_rep_w_prompt \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    234\u001b[0m mask_w_prompt \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/dev/hakaton-gagarin-sentiment_interface/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/hakaton-gagarin-sentiment_interface/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/hakaton-gagarin-sentiment_interface/.venv/lib/python3.11/site-packages/gliner/modules/token_rep.py:39\u001b[0m, in \u001b[0;36mTokenRepLayer.forward\u001b[0;34m(self, tokens, lengths)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens: List[List[\u001b[38;5;28mstr\u001b[39m]], lengths: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 39\u001b[0m     token_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_word_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprojection\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     42\u001b[0m         token_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojection(token_embeddings)\n",
      "File \u001b[0;32m~/dev/hakaton-gagarin-sentiment_interface/.venv/lib/python3.11/site-packages/gliner/modules/token_rep.py:52\u001b[0m, in \u001b[0;36mTokenRepLayer.compute_word_embedding\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_word_embedding\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens):\n\u001b[1;32m     51\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [Sentence(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     token_embeddings \u001b[38;5;241m=\u001b[39m pad_sequence([torch\u001b[38;5;241m.\u001b[39mstack([t\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m k]) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m sentences], batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m token_embeddings\n",
      "File \u001b[0;32m~/dev/hakaton-gagarin-sentiment_interface/.venv/lib/python3.11/site-packages/flair/embeddings/base.py:50\u001b[0m, in \u001b[0;36mEmbeddings.embed\u001b[0;34m(self, data_points)\u001b[0m\n\u001b[1;32m     47\u001b[0m     data_points \u001b[38;5;241m=\u001b[39m [data_points]\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_everything_embedded(data_points):\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_embeddings_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data_points\n",
      "File \u001b[0;32m~/dev/hakaton-gagarin-sentiment_interface/.venv/lib/python3.11/site-packages/flair/embeddings/transformer.py:705\u001b[0m, in \u001b[0;36mTransformerBaseEmbeddings._add_embeddings_internal\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    703\u001b[0m gradient_context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39menable_grad() \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfine_tune \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining) \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m gradient_context:\n\u001b[0;32m--> 705\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocument_embedding:\n\u001b[1;32m    708\u001b[0m     document_embedding \u001b[38;5;241m=\u001b[39m embeddings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/dev/hakaton-gagarin-sentiment_interface/.venv/lib/python3.11/site-packages/flair/embeddings/transformer.py:1424\u001b[0m, in \u001b[0;36mTransformerEmbeddings._forward_tensors\u001b[0;34m(self, tensors)\u001b[0m\n\u001b[1;32m   1423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_tensors\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensors) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m-> 1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/hakaton-gagarin-sentiment_interface/.venv/lib/python3.11/site-packages/flair/embeddings/transformer.py:1324\u001b[0m, in \u001b[0;36mTransformerEmbeddings.forward\u001b[0;34m(self, input_ids, sub_token_lengths, token_lengths, attention_mask, overflow_to_sample_mapping, word_ids, langs, bbox, pixel_values)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pixel_values\n\u001b[0;32m-> 1324\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;66;03m# make the tuple a tensor; makes working with it easier.\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(hidden_states)\n",
      "File \u001b[0;32m~/dev/hakaton-gagarin-sentiment_interface/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/hakaton-gagarin-sentiment_interface/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/hakaton-gagarin-sentiment_interface/.venv/lib/python3.11/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1070\u001b[0m, in \u001b[0;36mDebertaV2Model.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1060\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m   1062\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1063\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1064\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1067\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1068\u001b[0m )\n\u001b[0;32m-> 1070\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1077\u001b[0m encoded_layers \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/dev/hakaton-gagarin-sentiment_interface/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/hakaton-gagarin-sentiment_interface/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/hakaton-gagarin-sentiment_interface/.venv/lib/python3.11/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:514\u001b[0m, in \u001b[0;36mDebertaV2Encoder.forward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[1;32m    504\u001b[0m     output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    505\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    506\u001b[0m         next_kv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         output_attentions,\n\u001b[1;32m    512\u001b[0m     )\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 514\u001b[0m     output_states \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnext_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    524\u001b[0m     output_states, att_m \u001b[38;5;241m=\u001b[39m output_states\n",
      "File \u001b[0;32m~/dev/hakaton-gagarin-sentiment_interface/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/hakaton-gagarin-sentiment_interface/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/hakaton-gagarin-sentiment_interface/.venv/lib/python3.11/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:373\u001b[0m, in \u001b[0;36mDebertaV2Layer.forward\u001b[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    371\u001b[0m     attention_output, att_matrix \u001b[38;5;241m=\u001b[39m attention_output\n\u001b[1;32m    372\u001b[0m intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 373\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (layer_output, att_matrix)\n",
      "File \u001b[0;32m~/dev/hakaton-gagarin-sentiment_interface/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/hakaton-gagarin-sentiment_interface/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/hakaton-gagarin-sentiment_interface/.venv/lib/python3.11/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:339\u001b[0m, in \u001b[0;36mDebertaV2Output.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states, input_tensor):\n\u001b[0;32m--> 339\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    341\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m~/dev/hakaton-gagarin-sentiment_interface/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/hakaton-gagarin-sentiment_interface/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/hakaton-gagarin-sentiment_interface/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = dict()\n",
    "ind = 0\n",
    "for _, row in tqdm(sentiment_texts_df.iterrows(), total=len(sentiment_texts_df)):\n",
    "    if ind > 500:\n",
    "        break\n",
    "    message_id = row[\"MessageID\"]\n",
    "    message_text = row[\"MessageText\"]\n",
    "    if isinstance(message_text, list):\n",
    "        message_text = message_text[0]\n",
    "        message_id = message_id[0]\n",
    "    labels = [\"Company\"]\n",
    "    entities = model.predict_entities(message_text, labels=labels)\n",
    "    for entity in entities:\n",
    "        entity_text = entity[\"text\"]\n",
    "        found_issuer_id = namings.apply(\n",
    "            lambda x: entity_text.lower() in [s.lower() for s in x]\n",
    "        ).argmax()\n",
    "        if found_issuer_id > 0:\n",
    "            results.setdefault(message_id, set()).add(found_issuer_id + 1)\n",
    "    ind += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_results = dict()\n",
    "for x in sentiment_texts_df[[\"MessageID\", \"issuerid\"]].to_numpy():\n",
    "    mid = x[0]\n",
    "    if isinstance(mid, list):\n",
    "        mid = mid[0]\n",
    "    real_results.setdefault(mid, set()).update(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score = []\n",
    "iids: set\n",
    "for mid, iids in real_results.items():\n",
    "    acc_sc = max(len(iids.intersection(results[mid] if mid in results else {})), 1) / len(iids)\n",
    "    if mid in results:\n",
    "        accuracy_score.append(acc_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9133274301456119"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MessageID</th>\n",
       "      <th>ChannelID</th>\n",
       "      <th>issuerid</th>\n",
       "      <th>SentimentScore</th>\n",
       "      <th>DateAdded</th>\n",
       "      <th>DatePosted</th>\n",
       "      <th>MessageText</th>\n",
       "      <th>IsForward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>525</td>\n",
       "      <td>1564218803</td>\n",
       "      <td>[26]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>2023-06-02 19:25:01</td>\n",
       "      <td>2023-06-02 19:09:06</td>\n",
       "      <td>Результаты фондов под управлением УК Арикапита...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  MessageID   ChannelID issuerid SentimentScore            DateAdded  \\\n",
       "0       525  1564218803     [26]            [5]  2023-06-02 19:25:01   \n",
       "\n",
       "            DatePosted                                        MessageText  \\\n",
       "0  2023-06-02 19:09:06  Результаты фондов под управлением УК Арикапита...   \n",
       "\n",
       "  IsForward  \n",
       "0     False  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_texts_df[sentiment_texts_df.MessageID == 525]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{525: {4, 7, 26, 48, 56, 111, 112, 163},\n",
       " 559: {129},\n",
       " 561: {99},\n",
       " 562: {90, 99, 116, 152},\n",
       " 567: {129},\n",
       " 581: {90, 99, 116, 152},\n",
       " 609: {111, 160},\n",
       " 621: {7, 111},\n",
       " 622: {90, 99, 116, 152},\n",
       " 623: {7},\n",
       " 641: {111, 160},\n",
       " 648: {111, 115, 163},\n",
       " 654: {236},\n",
       " 659: {90, 99, 116, 152},\n",
       " 672: {90, 99, 116, 152},\n",
       " 674: {111, 160},\n",
       " 679: {90, 99, 116, 152},\n",
       " 733: {116},\n",
       " 772: {91, 100, 142},\n",
       " 779: {7, 142, 241},\n",
       " 1236: {235},\n",
       " 1275: {4,\n",
       "  9,\n",
       "  11,\n",
       "  22,\n",
       "  24,\n",
       "  48,\n",
       "  62,\n",
       "  67,\n",
       "  79,\n",
       "  86,\n",
       "  99,\n",
       "  111,\n",
       "  112,\n",
       "  113,\n",
       "  114,\n",
       "  115,\n",
       "  116,\n",
       "  127,\n",
       "  129,\n",
       "  152,\n",
       "  153,\n",
       "  163,\n",
       "  185,\n",
       "  187,\n",
       "  197,\n",
       "  199,\n",
       "  225,\n",
       "  227,\n",
       "  237},\n",
       " 1287: {115},\n",
       " 1344: {116},\n",
       " 1379: {235},\n",
       " 1380: {112},\n",
       " 1381: {112},\n",
       " 1394: {152},\n",
       " 1395: {152},\n",
       " 1411: {32},\n",
       " 1423: {48},\n",
       " 1431: {53, 56, 241},\n",
       " 1436: {7},\n",
       " 1437: {7},\n",
       " 1440: {4, 58, 100, 111, 116, 127, 129, 150, 166, 199},\n",
       " 1442: {4, 11, 24, 53, 90, 100, 116, 152, 187, 236},\n",
       " 1451: {48},\n",
       " 1454: {61},\n",
       " 1457: {236},\n",
       " 1482: {61},\n",
       " 1485: {4,\n",
       "  24,\n",
       "  36,\n",
       "  53,\n",
       "  86,\n",
       "  90,\n",
       "  103,\n",
       "  111,\n",
       "  112,\n",
       "  115,\n",
       "  127,\n",
       "  150,\n",
       "  152,\n",
       "  160,\n",
       "  163,\n",
       "  187,\n",
       "  239},\n",
       " 1487: {74, 158},\n",
       " 1491: {150, 236},\n",
       " 1492: {236},\n",
       " 1494: {56},\n",
       " 1508: {163},\n",
       " 1521: {223, 225, 236},\n",
       " 1525: {127},\n",
       " 1526: {112},\n",
       " 1546: {221},\n",
       " 1552: {112},\n",
       " 1554: {111},\n",
       " 1583: {100, 223, 236},\n",
       " 1601: {90, 116},\n",
       " 1605: {240},\n",
       " 1610: {116},\n",
       " 1615: {4, 150, 199},\n",
       " 1620: {112},\n",
       " 1627: {26, 100, 204},\n",
       " 1632: {4},\n",
       " 1641: {115},\n",
       " 1663: {157},\n",
       " 1664: {4, 11, 53, 58, 111, 166, 199, 250},\n",
       " 1667: {53},\n",
       " 1671: {111},\n",
       " 1673: {116},\n",
       " 1681: {142},\n",
       " 1682: {142},\n",
       " 1686: {26},\n",
       " 1694: {236},\n",
       " 1695: {236},\n",
       " 1696: {236},\n",
       " 1727: {142},\n",
       " 1730: {160},\n",
       " 1741: {53},\n",
       " 1905: {33},\n",
       " 1943: {115, 225},\n",
       " 1984: {223},\n",
       " 1998: {116},\n",
       " 2004: {112},\n",
       " 2063: {160},\n",
       " 2071: {240},\n",
       " 2077: {237, 240, 245},\n",
       " 2088: {160},\n",
       " 2091: {163},\n",
       " 2101: {223, 225, 236},\n",
       " 2115: {150},\n",
       " 2120: {231},\n",
       " 2131: {127, 150, 235},\n",
       " 2132: {11, 48, 61, 127},\n",
       " 2140: {9, 107, 127, 128, 184, 235},\n",
       " 2142: {221, 223, 225, 236, 237, 245},\n",
       " 2144: {235},\n",
       " 2146: {157, 220},\n",
       " 2149: {219},\n",
       " 2150: {236},\n",
       " 2158: {236},\n",
       " 2165: {235, 236},\n",
       " 2176: {112},\n",
       " 2180: {245},\n",
       " 2183: {112, 115},\n",
       " 2187: {5, 33, 99, 115, 177, 247},\n",
       " 2188: {150},\n",
       " 2190: {90},\n",
       " 2195: {142},\n",
       " 2200: {236},\n",
       " 2212: {7,\n",
       "  9,\n",
       "  22,\n",
       "  24,\n",
       "  26,\n",
       "  48,\n",
       "  86,\n",
       "  111,\n",
       "  112,\n",
       "  113,\n",
       "  115,\n",
       "  142,\n",
       "  160,\n",
       "  163,\n",
       "  175,\n",
       "  187,\n",
       "  221,\n",
       "  225,\n",
       "  227,\n",
       "  237,\n",
       "  244,\n",
       "  245},\n",
       " 2214: {231},\n",
       " 2220: {142, 223, 236, 237, 240, 241},\n",
       " 2223: {4, 48, 53, 100, 115, 127, 150, 231, 236},\n",
       " 2231: {157},\n",
       " 2232: {111, 112, 157, 160, 163, 184, 221},\n",
       " 2234: {129},\n",
       " 2236: {220},\n",
       " 2238: {244},\n",
       " 2242: {7, 185},\n",
       " 2243: {4},\n",
       " 2258: {157},\n",
       " 2260: {235},\n",
       " 2264: {26, 61, 100},\n",
       " 2272: {219, 220, 221, 225, 236, 237, 240},\n",
       " 2273: {153, 204},\n",
       " 2276: {36},\n",
       " 2283: {204},\n",
       " 2290: {26, 100, 204},\n",
       " 2323: {127, 150},\n",
       " 2324: {33},\n",
       " 2332: {61, 111, 112, 118, 157, 163, 220},\n",
       " 2338: {225},\n",
       " 2340: {111, 150, 249},\n",
       " 2341: {187},\n",
       " 2343: {7, 48, 111, 112, 160, 163, 187, 225, 235},\n",
       " 2345: {118, 157, 220},\n",
       " 2396: {4, 153},\n",
       " 2423: {32, 52, 56, 111, 112, 115, 163, 220, 227, 236},\n",
       " 2461: {90, 116, 152},\n",
       " 2464: {111},\n",
       " 2467: {33},\n",
       " 2477: {187},\n",
       " 2483: {48, 115, 157, 158, 164, 219, 220},\n",
       " 2494: {53, 61, 111, 112, 118, 163, 175, 220, 225, 227},\n",
       " 2503: {237},\n",
       " 2513: {61, 111, 112, 115, 160},\n",
       " 2521: {204},\n",
       " 2528: {99, 160, 185, 199, 231, 245},\n",
       " 2537: {32, 225},\n",
       " 2545: {48},\n",
       " 2581: {99, 225},\n",
       " 2599: {7, 245},\n",
       " 2608: {142},\n",
       " 2609: {117, 153, 235, 247},\n",
       " 2613: {7, 48, 56, 111, 163, 223},\n",
       " 2622: {4, 142},\n",
       " 2629: {57, 231},\n",
       " 2645: {103},\n",
       " 2650: {99, 115, 235},\n",
       " 2679: {160},\n",
       " 2686: {99, 129},\n",
       " 2695: {248, 249},\n",
       " 2704: {245},\n",
       " 2711: {48, 53, 111, 112, 152},\n",
       " 2718: {56, 245},\n",
       " 2726: {100, 127, 142, 150, 153, 220, 247},\n",
       " 2730: {53, 225, 227},\n",
       " 2738: {115},\n",
       " 2739: {61, 111},\n",
       " 2747: {153, 247},\n",
       " 2750: {227},\n",
       " 2754: {48},\n",
       " 2765: {100, 150},\n",
       " 2766: {247},\n",
       " 2767: {7, 48, 112, 225, 236},\n",
       " 2768: {48, 99, 111, 112, 152, 199, 235, 247},\n",
       " 2769: {4},\n",
       " 2837: {227},\n",
       " 2847: {57},\n",
       " 2848: {157},\n",
       " 2875: {90},\n",
       " 2877: {236},\n",
       " 2903: {100},\n",
       " 2917: {99, 129},\n",
       " 2922: {223},\n",
       " 2924: {127},\n",
       " 2926: {129},\n",
       " 2938: {112},\n",
       " 2951: {115},\n",
       " 2957: {220},\n",
       " 2962: {136},\n",
       " 2968: {240},\n",
       " 2970: {204},\n",
       " 2974: {57, 115},\n",
       " 2976: {204},\n",
       " 2987: {26},\n",
       " 2994: {185},\n",
       " 3004: {7, 26, 111, 125, 221},\n",
       " 3011: {100, 204},\n",
       " 3018: {4},\n",
       " 3021: {7, 48, 53, 112},\n",
       " 3037: {7, 90, 116, 127, 152, 187, 225},\n",
       " 3039: {90},\n",
       " 3040: {7, 48, 111, 112, 115, 163, 231, 236},\n",
       " 3046: {48},\n",
       " 3052: {48, 150},\n",
       " 3053: {48, 90, 111, 115, 150, 152, 236},\n",
       " 3061: {185},\n",
       " 3067: {7},\n",
       " 3078: {227},\n",
       " 3081: {219},\n",
       " 3092: {7, 56, 90, 185, 219, 231, 236, 241},\n",
       " 3106: {163},\n",
       " 3123: {236},\n",
       " 3144: {235, 236},\n",
       " 3168: {187},\n",
       " 3180: {48, 107, 113, 167, 177, 184, 187, 215, 225, 244},\n",
       " 3191: {99},\n",
       " 3192: {185},\n",
       " 3198: {26, 57, 100, 111},\n",
       " 3200: {99},\n",
       " 3210: {231},\n",
       " 3212: {11, 32},\n",
       " 3213: {48, 53, 100, 111, 112, 115, 236},\n",
       " 3224: {160},\n",
       " 3239: {26, 100},\n",
       " 3251: {61},\n",
       " 3267: {48},\n",
       " 3296: {235},\n",
       " 3297: {235},\n",
       " 3300: {127, 235},\n",
       " 3307: {118},\n",
       " 3314: {4},\n",
       " 3348: {231},\n",
       " 3375: {236},\n",
       " 3389: {127, 153, 235},\n",
       " 3392: {225},\n",
       " 3398: {58},\n",
       " 3399: {227},\n",
       " 3401: {235},\n",
       " 3402: {235},\n",
       " 3403: {219},\n",
       " 3405: {32},\n",
       " 3411: {142},\n",
       " 3414: {150},\n",
       " 3419: {157},\n",
       " 3421: {231},\n",
       " 3437: {115},\n",
       " 3441: {36},\n",
       " 3443: {242},\n",
       " 3455: {67, 180, 183, 194, 198},\n",
       " 3459: {33},\n",
       " 3462: {236},\n",
       " 3473: {100},\n",
       " 3481: {129},\n",
       " 3494: {187},\n",
       " 3498: {103},\n",
       " 3499: {47, 61, 99, 157, 199, 204},\n",
       " 3500: {231},\n",
       " 3502: {221},\n",
       " 3503: {231},\n",
       " 3504: {111},\n",
       " 3510: {112},\n",
       " 3512: {187},\n",
       " 3522: {127},\n",
       " 3523: {225},\n",
       " 3524: {225},\n",
       " 3525: {221, 236, 240},\n",
       " 3526: {223},\n",
       " 3527: {235},\n",
       " 3528: {56, 57},\n",
       " 3529: {225},\n",
       " 3531: {90},\n",
       " 3535: {157},\n",
       " 3542: {118, 158, 219, 225},\n",
       " 3543: {237},\n",
       " 3544: {7},\n",
       " 3545: {157}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
